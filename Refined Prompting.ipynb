{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc086d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "from io import StringIO\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c1cb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://hostedvm.com/lmstudio/v1/chat/completions\" # address of the VM\n",
    "AUTH = (\"User123\", \"Password123\")  # Change to your login credentials\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "# Configuration\n",
    "DOC_IDS_PER_CHUNK = 1  # Adjust based on your context window limits\n",
    "OUTPUT_DIR = \"analysis_results\"\n",
    "\n",
    "def extract_csv_content(reply_text):\n",
    "    \"\"\"Extract CSV content from markdown fencing in LLM response\"\"\"\n",
    "    csv_blocks = re.findall(r'```(?:csv)?\\n(.*?)\\n```', reply_text, re.DOTALL)\n",
    "    \n",
    "    if csv_blocks:\n",
    "        csv_content = csv_blocks[0].strip()\n",
    "        valid_lines = [line for line in csv_content.split('\\n') \n",
    "                      if line.count(',') == 1 or line.startswith('Doc_id')]\n",
    "        return '\\n'.join(valid_lines)\n",
    "    \n",
    "    # Fallback: Try to find CSV lines without fencing\n",
    "    valid_lines = [line for line in reply_text.split('\\n') \n",
    "                 if line.count(',') == 1 and line.strip()]\n",
    "    if len(valid_lines) >= 1:\n",
    "        return '\\n'.join(valid_lines)\n",
    "    \n",
    "    return None\n",
    "\n",
    "def validate_csv(df):\n",
    "    \"\"\"Validate the structure and content of the parsed CSV\"\"\"\n",
    "    required_columns = ['Doc_id', 'Vagueness_Score']\n",
    "    \n",
    "    if not all(col in df.columns for col in required_columns):\n",
    "        raise ValueError(f\"Missing required columns. Found: {df.columns.tolist()}\")\n",
    "    \n",
    "    if not pd.api.types.is_numeric_dtype(df['Vagueness_Score']):\n",
    "        raise ValueError(\"Vagueness_Score must be numeric\")\n",
    "    \n",
    "    if (df['Vagueness_Score'] < 0).any() or (df['Vagueness_Score'] > 1).any():\n",
    "        raise ValueError(\"Scores must be between 0 and 1\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def process_chunk(chunk_df, chunk_number, total_chunks):\n",
    "    \"\"\"Process a single chunk of data through the LLM API with retries\"\"\"\n",
    "    MAX_RETRIES = 5  # Maximum number of retry attempts\n",
    "    RETRY_DELAY = 5  # Seconds to wait between retries\n",
    "    TIMEOUT = 60     # Timeout for each API call\n",
    "\n",
    "    records_json = chunk_df.to_dict(orient=\"records\")\n",
    "    json_text = json.dumps(records_json, indent=2)\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    You are an expert financial analyst. Your task is to evaluate the clarity and directness of company communications in earnings call Q&A sections.\n",
    "\n",
    "    You will be provided with a JSON array. Each element in the array is a dictionary representing one turn of speech from the Q&A section of one or more companies' earnings call.\n",
    "    The data contains transcripts with the following columns:\n",
    "    - \"Doc_id\": A unique identifier for each distinct earnings call.\n",
    "    - \"Speech\": The transcribed text of what was said.\n",
    "    - \"Speaker_Type\": Indicates the speaker: 1 for a company representative (e.g., CEO, CFO), 2 for an external analyst or questioner.\n",
    "\n",
    "    **Your Primary Goal:**\n",
    "    For EACH unique \"Doc_id\" present in the provided data, you must:\n",
    "    1.  Analyze ALL company responses (Speaker_Type 1) to questions from analysts (Speaker_Type 2) within that specific \"Doc_id\".\n",
    "    2.  Generate a single \"Vagueness_Score\" that reflects the overall level of vagueness from the company's side during the Q&A of that entire earnings call.\n",
    "\n",
    "    **\"Vagueness_Score\" Definition:**\n",
    "    - A float between 0.0000 and 1.0000 (inclusive, 4 decimal places).\n",
    "    - 0.0000: Perfectly clear, direct, specific, and comprehensive answers from the company.\n",
    "    - 1.0000: Extremely vague, evasive, non-committal, or unclear answers from the company.\n",
    "\n",
    "    **Detailed Criteria for Assessing Vagueness (Consider these collectively for all company answers within a single \"Doc_id\"):**\n",
    "\n",
    "    1.  **Directness & Relevance:**\n",
    "        * How directly does the company (Speaker_Type 1) address the specific questions posed by analysts (Speaker_Type 2)?\n",
    "        * Note instances of deflection, topic changes, or answering a different question than asked.\n",
    "        * *Higher vagueness score for frequent indirectness or irrelevant responses.*\n",
    "\n",
    "    2.  **Specificity & Substance:**\n",
    "        * Do company answers provide concrete details, figures, timelines, or specific examples when appropriate and reasonably expected?\n",
    "        * Or are they characterized by generalizations, abstractions, or a lack of substantive information?\n",
    "        * *Higher vagueness score for pervasive lack of specificity.*\n",
    "\n",
    "    3.  **Hedging & Qualifying Language:**\n",
    "        * Identify the presence and frequency of words/phrases indicating uncertainty or reservation (e.g., \"might,\" \"could,\" \"possibly,\" \"we believe,\" \"generally,\" \"it seems,\" \"potentially,\" \"around,\" \"approximately,\" \"sort of,\" \"perhaps,\" \"we expect,\" \"we aim to,\" \"feels like\").\n",
    "        * While some caution is normal, evaluate if such language is used excessively to obscure meaning or avoid commitment.\n",
    "        * *Higher vagueness score if hedging is heavy and reduces clarity.*\n",
    "\n",
    "    4.  **Evasiveness & Non-Answers:**\n",
    "        * Are there clear instances of the company refusing to answer, or providing statements that don't substantively address the question (e.g., \"We don't comment on that,\" \"That's proprietary,\" \"We are continuously evaluating...\" without further useful context)?\n",
    "        * Assess if such non-answers are frequent or if they appear to be part of a pattern of avoidance for that call.\n",
    "        * *Higher vagueness score for patterns of evasion or frequent non-answers.*\n",
    "\n",
    "    5.  **Clarity of Language & Phrasing:**\n",
    "        * Is the language used by the company clear, precise, and easily understandable?\n",
    "        * Or is it laden with unexplained jargon, overly complex sentence structures, or ambiguous phrasing that could intentionally or unintentionally obscure the meaning?\n",
    "        * *Higher vagueness score for opaque or unnecessarily complex language.*\n",
    "\n",
    "    6.  **Level of Commitment:**\n",
    "        * Do the company's answers demonstrate a clear position, plan, or commitment where appropriate?\n",
    "        * Or do they consistently remain non-committal, leaving significant ambiguity about the company's intentions or outlook?\n",
    "        * *Higher vagueness score for a pattern of non-committal responses.*\n",
    "\n",
    "    **Processing Instructions (Follow these steps for the unique \"Doc_id\"):**\n",
    "    A. Focus only on the data pertaining to the \"Doc_id\".\n",
    "    B. Within that \"Doc_id\", identify all question-answer exchanges. A question is typically (not always), a \"Speech\" from \"Speaker_Type\" 2, followed by one or more \"Speech\" segments from \"Speaker_Type\" 1 which constitute the company's answer.\n",
    "    C. For each company answer (which may span multiple \"Speech\" entries from Speaker_Type 1), evaluate its clarity and directness based on ALL the criteria listed above.\n",
    "    D. After evaluating all company answers within that single \"Doc_id\", synthesize your observations into ONE holistic \"Vagueness_Score\" for that entire earnings call. This score should represent your overall assessment of the company's communication style in that Q&A session.\n",
    "    \n",
    "    **Output Format STRICTLY REQUIRED:**\n",
    "    - Only output a CSV formatted string. NO other text, explanation, or commentary.\n",
    "    - The CSV string must have a header row: \"Doc_id\",\"Vagueness_Score\"\n",
    "\n",
    "    Example Output:\n",
    "    \"Doc_id\",\"Vagueness_Score\"\n",
    "    \"12345\",\"0.1234\"\n",
    "\n",
    "    Data:\n",
    "    {json_text}\n",
    "    \"\"\"\n",
    "    \n",
    "    payload = {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    for attempt in range(MAX_RETRIES + 1):\n",
    "        try:\n",
    "            response = requests.post(URL, json=payload, headers=HEADERS, \n",
    "                                   auth=AUTH, timeout=TIMEOUT)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # If we got a successful response, process it\n",
    "            reply_text = response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "            cleaned_csv = extract_csv_content(reply_text)\n",
    "            \n",
    "            if not cleaned_csv:\n",
    "                raise ValueError(\"No valid CSV data found in response\")\n",
    "                \n",
    "            df = pd.read_csv(StringIO(cleaned_csv))\n",
    "            df = validate_csv(df)\n",
    "            \n",
    "            if attempt > 0:  # Only show retry success if actually retried\n",
    "                print(f\"Retry #{attempt} successful for chunk {chunk_number}\")\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            if attempt < MAX_RETRIES:\n",
    "                print(f\"Attempt {attempt + 1}/{MAX_RETRIES} failed for chunk {chunk_number}: {str(e)}\")\n",
    "                print(f\"Retrying in {RETRY_DELAY} seconds...\")\n",
    "                time.sleep(RETRY_DELAY * (attempt + 1))\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"Final attempt failed for chunk {chunk_number}: {str(e)}\")\n",
    "                return None\n",
    "            \n",
    "def save_intermediate_results(results_list, chunk_number, total_processed_docs):\n",
    "    \"\"\"Saves the currently collected results to an intermediate CSV file.\"\"\"\n",
    "    if not results_list:\n",
    "        print(f\"No results to save at intermediate point (after chunk {chunk_number}).\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nSaving intermediate results after chunk {chunk_number}...\")\n",
    "    try:\n",
    "        # Consolidate results collected so far\n",
    "        intermediate_df = pd.concat(results_list, ignore_index=True)\n",
    "        # Apply the same de-duplication and sorting as the final step\n",
    "        intermediate_df = intermediate_df.drop_duplicates('Doc_id').sort_values('Doc_id')\n",
    "\n",
    "        os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        # Create a descriptive filename for the intermediate file\n",
    "        output_path = os.path.join(OUTPUT_DIR, f\"vagueness_scores_intermediate_upto_chunk_{chunk_number}_{timestamp}.csv\")\n",
    "        intermediate_df.to_csv(output_path, index=False)\n",
    "        print(f\"Intermediate results for {len(intermediate_df)} unique Doc_ids (out of {total_processed_docs} processed) saved to: {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving intermediate results at chunk {chunk_number}: {e}\")\n",
    "\n",
    "def main():\n",
    "    # Define the interval for intermediate saves\n",
    "    INTERMEDIATE_SAVE_INTERVAL = 200 # Save every x chunks\n",
    "\n",
    "    # Load and prepare data\n",
    "    try:\n",
    "        llm_data = pd.read_csv(\"llm_data3.csv\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: data not found. Please ensure the file exists in the correct location.\")\n",
    "        return\n",
    "        \n",
    "    unique_doc_ids = llm_data['Doc_id'].unique().tolist()\n",
    "    doc_id_chunks = [unique_doc_ids[i:i+DOC_IDS_PER_CHUNK]\n",
    "                   for i in range(0, len(unique_doc_ids), DOC_IDS_PER_CHUNK)]\n",
    "\n",
    "    # Process chunks\n",
    "    all_results = []\n",
    "    total_chunks = len(doc_id_chunks)\n",
    "    processed_doc_ids_count = 0 # keep track of unique doc_ids in all_results for logging\n",
    "\n",
    "    for chunk_idx, doc_ids in enumerate(doc_id_chunks, 1):\n",
    "        chunk_df = llm_data[llm_data['Doc_id'].isin(doc_ids)]\n",
    "\n",
    "        print(f\"\\nProcessing chunk {chunk_idx}/{total_chunks} \"\n",
    "              f\"({len(chunk_df)} rows, {len(chunk_df['Doc_id'].unique())} Doc_ids)\")\n",
    "\n",
    "        start_time = time.time()\n",
    "        result_df = process_chunk(chunk_df, chunk_idx, total_chunks)\n",
    "        elapsed_time = time.time() - start_time\n",
    "\n",
    "        if result_df is not None:\n",
    "            all_results.append(result_df)\n",
    "            # Update a running count of unique Doc_ids for more accurate intermediate save messages\n",
    "            print(f\"Completed chunk {chunk_idx}/{total_chunks}: \"\n",
    "                  f\"{len(result_df)} Doc_ids in {elapsed_time:.2f}s. \"\n",
    "                  f\"Total successful results accumulated: {sum(len(df) for df in all_results)} Doc_id entries.\")\n",
    "\n",
    "            # Fail-Safe:\n",
    "            # Save if it's an interval chunk AND it's not the very last chunk\n",
    "            if chunk_idx % INTERMEDIATE_SAVE_INTERVAL == 0 and chunk_idx < total_chunks:\n",
    "                # For accurate count in save_intermediate_results, we can pass the current count of unique doc_ids\n",
    "                # by temporarily concatenating and counting.\n",
    "                temp_concat_df = pd.concat(all_results, ignore_index=True)\n",
    "                current_unique_docs = temp_concat_df['Doc_id'].nunique()\n",
    "                save_intermediate_results(all_results, chunk_idx, current_unique_docs)\n",
    "            # End Fail-Safe\n",
    "\n",
    "        else:\n",
    "            print(f\"Failed chunk {chunk_idx}/{total_chunks}\")\n",
    "\n",
    "    # Combine and save final results\n",
    "    if all_results:\n",
    "        print(\"\\nConsolidating all results for final save...\")\n",
    "        final_df = pd.concat(all_results, ignore_index=True)\n",
    "        final_df = final_df.drop_duplicates('Doc_id').sort_values('Doc_id')\n",
    "\n",
    "        os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        # Make final filename distinct\n",
    "        output_path = os.path.join(OUTPUT_DIR, f\"vagueness_scores_FINAL_{timestamp}.csv\")\n",
    "        final_df.to_csv(output_path, index=False)\n",
    "        print(f\"\\nSuccess! Processed {len(final_df)} unique Doc_ids in total.\")\n",
    "        print(f\"Final results saved to: {output_path}\")\n",
    "    else:\n",
    "        print(\"\\nNo results were successfully processed in total.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d43a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "URL = \"https://hostedvm.com/lmstudio/v1/chat/completions\" # address of the VM\n",
    "AUTH = (\"User123\", \"Password123\")  # Change to your login credentials\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "# Configuration\n",
    "DOC_IDS_PER_CHUNK = 1  # Adjust based on your context window limits\n",
    "OUTPUT_DIR = \"analysis_results_clarity\" \n",
    "\n",
    "def extract_csv_content(reply_text):\n",
    "    \"\"\"Extract CSV content from markdown fencing in LLM response\"\"\"\n",
    "    # Look for CSV blocks between triple backticks\n",
    "    csv_blocks = re.findall(r'```(?:csv)?\\n(.*?)\\n```', reply_text, re.DOTALL)\n",
    "    \n",
    "    if csv_blocks:\n",
    "        # Use the first found CSV block\n",
    "        csv_content = csv_blocks[0].strip()\n",
    "        # Filter lines to ensure proper CSV format\n",
    "        valid_lines = [line for line in csv_content.split('\\n') \n",
    "                      if line.count(',') == 1 or line.startswith('Doc_id')]\n",
    "        return '\\n'.join(valid_lines)\n",
    "    \n",
    "    # Fallback: Try to find CSV lines without fencing\n",
    "    valid_lines = [line for line in reply_text.split('\\n') \n",
    "                 if line.count(',') == 1 and line.strip()]\n",
    "    if len(valid_lines) >= 1:\n",
    "        return '\\n'.join(valid_lines)\n",
    "    \n",
    "    return None\n",
    "\n",
    "def validate_csv(df):\n",
    "    \"\"\"Validate the structure and content of the parsed CSV\"\"\"\n",
    "    required_columns = ['Doc_id', 'Clarity_Score']\n",
    "    \n",
    "    if not all(col in df.columns for col in required_columns):\n",
    "        raise ValueError(f\"Missing required columns. Found: {df.columns.tolist()}\")\n",
    "    \n",
    "    if not pd.api.types.is_numeric_dtype(df['Clarity_Score']):\n",
    "        raise ValueError(\"Clarity_Score must be numeric\")\n",
    "    \n",
    "    if (df['Clarity_Score'] < 0).any() or (df['Clarity_Score'] > 1).any():\n",
    "        raise ValueError(\"Clarity_Score must be between 0 and 1\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def process_chunk(chunk_df, chunk_number, total_chunks):\n",
    "    \"\"\"Process a single chunk of data through the LLM API with retries\"\"\"\n",
    "    MAX_RETRIES = 5\n",
    "    RETRY_DELAY = 5\n",
    "    TIMEOUT = 60\n",
    "\n",
    "    records_json = chunk_df.to_dict(orient=\"records\")\n",
    "    json_text = json.dumps(records_json, indent=2)\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    You are an expert financial analyst. Your task is to evaluate the clarity, precision, and directness of company communications in earnings call Q&A sections.\n",
    "\n",
    "    You will be provided with a JSON array. Each element in the array is a dictionary representing one turn of speech from the Q&A section of one or more companies' earnings call.\n",
    "    The data contains transcripts with the following columns:\n",
    "    - \"Doc_id\": A unique identifier for each distinct earnings call.\n",
    "    - \"Speech\": The transcribed text of what was said.\n",
    "    - \"Speaker_Type\": Indicates the speaker: 1 for a company representative (e.g., CEO, CFO), 2 for an external analyst or questioner.\n",
    "\n",
    "    **Your Primary Goal:**\n",
    "    For EACH unique \"Doc_id\" present in the provided data, you must:\n",
    "    1.  Analyze ALL company responses (Speaker_Type 1) to questions from analysts (Speaker_Type 2) within that specific \"Doc_id\".\n",
    "    2.  Generate a single \"Clarity_Score\" that reflects the overall level of clarity and precision from the company's side during the Q&A of that entire earnings call.\n",
    "\n",
    "    **\"Clarity_Score\" Definition:**\n",
    "    - A float between 0.0000 and 1.0000 (inclusive, 4 decimal places).\n",
    "    - 0.0000: Extremely unclear, evasive, non-committal, or ambiguous answers from the company.\n",
    "    - 1.0000: Perfectly clear, direct, specific, and comprehensive answers from the company.\n",
    "\n",
    "    **Detailed Criteria for Assessing Clarity (Consider these collectively for all company answers within a single \"Doc_id\"):**\n",
    "\n",
    "    1.  **Directness & Relevance:**\n",
    "        * How directly and relevantly does the company (Speaker_Type 1) address the specific questions posed by analysts (Speaker_Type 2)?\n",
    "        * Are answers focused and on-topic, avoiding deflection or changing the subject?\n",
    "        * *Higher clarity score for consistent directness and relevance.*\n",
    "\n",
    "    2.  **Specificity & Substance:**\n",
    "        * Do company answers provide concrete details, figures, timelines, or specific examples when appropriate and reasonably expected?\n",
    "        * Are they substantive and informative, rather than relying on generalizations or abstractions?\n",
    "        * *Higher clarity score for prevalent specificity and substantive information.*\n",
    "\n",
    "    3.  **Use of Hedging & Qualifying Language:**\n",
    "        * Identify the presence of words/phrases indicating uncertainty (e.g., \"might,\" \"could,\" \"possibly,\" \"we believe,\" \"generally,\" \"it seems,\" \"potentially,\" \"around,\" \"approximately,\" \"sort of,\" \"perhaps,\" \"we expect,\" \"we aim to,\" \"feels like\").\n",
    "        * Assess if the use of such language is appropriate for the context and doesn't excessively obscure meaning or avoid commitment. Some cautious language is normal.\n",
    "        * *Higher clarity score if qualifying language is used judiciously and doesn't detract from overall clarity; lower clarity score if hedging is heavy and significantly reduces precision or commitment.*\n",
    "\n",
    "    4.  **Willingness to Answer & Transparency:**\n",
    "        * Are questions generally answered substantively, or are there frequent refusals, evasions, or statements that don't meaningfully address the question (e.g., \"We don't comment on that,\" \"That's proprietary,\" \"We are continuously evaluating...\" without useful context)?\n",
    "        * While some non-answers are legitimate, assess if there's a pattern of avoidance.\n",
    "        * *Higher clarity score for a pattern of providing substantive answers and transparency; lower for frequent evasion or non-answers that lack justification.*\n",
    "\n",
    "    5.  **Clarity of Language & Phrasing:**\n",
    "        * Is the language used by the company clear, precise, and easily understandable?\n",
    "        * Is it free from unexplained jargon, overly complex sentence structures, or ambiguous phrasing?\n",
    "        * *Higher clarity score for clear, concise, and unambiguous language.*\n",
    "\n",
    "    6.  **Level of Commitment & Decisiveness:**\n",
    "        * Do the company's answers demonstrate a clear position, plan, or commitment where appropriate and reasonably expected?\n",
    "        * Do they convey decisiveness when the situation calls for it, rather than remaining consistently non-committal?\n",
    "        * *Higher clarity score for a pattern of clear commitment and decisiveness where warranted.*\n",
    "\n",
    "    **Processing Instructions (Follow these steps for the unique \"Doc_id\"):**\n",
    "    A. Focus only on the data pertaining to the \"Doc_id\".\n",
    "    B. Within that \"Doc_id\", identify all question-answer exchanges. A question is typically (not always), a \"Speech\" from \"Speaker_Type\" 2, followed by one or more \"Speech\" segments from \"Speaker_Type\" 1 which constitute the company's answer.\n",
    "    C. For each company answer (which may span multiple \"Speech\" entries from Speaker_Type 1), evaluate its clarity and directness based on ALL the criteria listed above.\n",
    "    D. After evaluating all company answers within that single \"Doc_id\", synthesize your observations into ONE holistic \"Clarity_Score\" for that entire earnings call. This score should represent your overall assessment of the company's communication style in that Q&A session.\n",
    "    \n",
    "    **Output Format STRICTLY REQUIRED:**\n",
    "    - Only output a CSV formatted string. NO other text, explanation, or commentary.\n",
    "    - The CSV string must have a header row: \"Doc_id\",\"Clarity_Score\"\n",
    "\n",
    "    Example Output:\n",
    "    \"Doc_id\",\"Clarity_Score\"\n",
    "    \"12345\",\"0.8765\"\n",
    "\n",
    "    Data:\n",
    "    {json_text}\n",
    "    \"\"\"\n",
    "    \n",
    "    payload = {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant specialized in financial text analysis, focusing on the clarity of communication.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    for attempt in range(MAX_RETRIES + 1):\n",
    "        try:\n",
    "            response = requests.post(URL, json=payload, headers=HEADERS, \n",
    "                                   auth=AUTH, timeout=TIMEOUT)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            reply_text = response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "            cleaned_csv = extract_csv_content(reply_text)\n",
    "            \n",
    "            if not cleaned_csv:\n",
    "                raise ValueError(\"No valid CSV data found in response\")\n",
    "                \n",
    "            df = pd.read_csv(StringIO(cleaned_csv))\n",
    "            df = validate_csv(df)\n",
    "            \n",
    "            if attempt > 0:\n",
    "                print(f\"Retry #{attempt} successful for chunk {chunk_number}\")\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            if attempt < MAX_RETRIES:\n",
    "                print(f\"Attempt {attempt + 1}/{MAX_RETRIES} failed for chunk {chunk_number}: {str(e)}\")\n",
    "                print(f\"Retrying in {RETRY_DELAY * (attempt + 1)} seconds...\")\n",
    "                time.sleep(RETRY_DELAY * (attempt + 1))\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"Final attempt failed for chunk {chunk_number}: {str(e)}\")\n",
    "                return None\n",
    "            \n",
    "def save_intermediate_results(results_list, chunk_number, total_processed_docs):\n",
    "    \"\"\"Saves the currently collected results to an intermediate CSV file.\"\"\"\n",
    "    if not results_list:\n",
    "        print(f\"No results to save at intermediate point (after chunk {chunk_number}).\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nSaving intermediate results after chunk {chunk_number}...\")\n",
    "    try:\n",
    "        intermediate_df = pd.concat(results_list, ignore_index=True)\n",
    "        intermediate_df = intermediate_df.drop_duplicates('Doc_id').sort_values('Doc_id')\n",
    "\n",
    "        os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_path = os.path.join(OUTPUT_DIR, f\"clarity_scores_intermediate_upto_chunk_{chunk_number}_{timestamp}.csv\")\n",
    "        intermediate_df.to_csv(output_path, index=False)\n",
    "        print(f\"Intermediate results for {len(intermediate_df)} unique Doc_ids (out of {total_processed_docs} processed) saved to: {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving intermediate results at chunk {chunk_number}: {e}\")\n",
    "\n",
    "def main():\n",
    "    INTERMEDIATE_SAVE_INTERVAL = 200\n",
    "\n",
    "    try:\n",
    "        llm_data = pd.read_csv(\"llm_data_sample2000.csv\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: llm_data3.csv not found. Please ensure the file exists in the correct location.\")\n",
    "        return\n",
    "        \n",
    "    unique_doc_ids = llm_data['Doc_id'].unique().tolist()\n",
    "    doc_id_chunks = [unique_doc_ids[i:i+DOC_IDS_PER_CHUNK]\n",
    "                   for i in range(0, len(unique_doc_ids), DOC_IDS_PER_CHUNK)]\n",
    "\n",
    "    all_results = []\n",
    "    total_chunks = len(doc_id_chunks)\n",
    "\n",
    "    for chunk_idx, doc_ids in enumerate(doc_id_chunks, 1):\n",
    "        chunk_df = llm_data[llm_data['Doc_id'].isin(doc_ids)]\n",
    "\n",
    "        print(f\"\\nProcessing chunk {chunk_idx}/{total_chunks} \"\n",
    "              f\"({len(chunk_df)} rows, {len(chunk_df['Doc_id'].unique())} Doc_ids) for clarity scores.\")\n",
    "\n",
    "        start_time = time.time()\n",
    "        result_df = process_chunk(chunk_df, chunk_idx, total_chunks)\n",
    "        elapsed_time = time.time() - start_time\n",
    "\n",
    "        if result_df is not None:\n",
    "            all_results.append(result_df)\n",
    "            print(f\"Completed chunk {chunk_idx}/{total_chunks}: \"\n",
    "                  f\"{len(result_df)} Doc_ids processed for clarity in {elapsed_time:.2f}s. \"\n",
    "                  f\"Total successful results accumulated: {sum(len(df) for df in all_results)} Doc_id entries.\")\n",
    "\n",
    "            if chunk_idx % INTERMEDIATE_SAVE_INTERVAL == 0 and chunk_idx < total_chunks:\n",
    "                temp_concat_df = pd.concat(all_results, ignore_index=True)\n",
    "                current_unique_docs = temp_concat_df['Doc_id'].nunique()\n",
    "                save_intermediate_results(all_results, chunk_idx, current_unique_docs)\n",
    "        else:\n",
    "            print(f\"Failed chunk {chunk_idx}/{total_chunks} for clarity score generation.\")\n",
    "\n",
    "    if all_results:\n",
    "        print(\"\\nConsolidating all clarity results for final save...\")\n",
    "        final_df = pd.concat(all_results, ignore_index=True)\n",
    "        final_df = final_df.drop_duplicates('Doc_id').sort_values('Doc_id')\n",
    "\n",
    "        os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_path = os.path.join(OUTPUT_DIR, f\"clarity_scores_FINAL_{timestamp}.csv\")\n",
    "        final_df.to_csv(output_path, index=False)\n",
    "        print(f\"\\nSuccess! Processed {len(final_df)} unique Doc_ids for clarity scores in total.\")\n",
    "        print(f\"Final clarity scores saved to: {output_path}\")\n",
    "    else:\n",
    "        print(\"\\nNo clarity score results were successfully processed in total.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
