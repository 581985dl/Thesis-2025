{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b3f7e00",
   "metadata": {},
   "source": [
    "Gunning fog index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c7d547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing documents to calculate Gunning Fog Index on combined company speech...\n",
      "Gunning Fog Index calculation complete for all documents.\n",
      "\n",
      "Gunning Fog Index scores (calculated on combined text per Doc_id) saved to 'doc_gunning_fog_scores_combined_fix.csv'\n",
      "\n",
      "First few rows of the output:\n",
      "    Doc_id  Average_Gunning_Fog\n",
      "0  1943275            10.718415\n",
      "1  2038813             9.877681\n",
      "2  2053228            15.148305\n",
      "3  2053230            13.077899\n",
      "4  2056621            13.453787\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import pyphen # For syllable counting\n",
    "import string\n",
    "import re\n",
    "import numpy as np # For np.nan\n",
    "\n",
    "# --- NLTK Resource Download Check (ensure 'punkt' is available) ---\n",
    "# (Assuming 'punkt' is already downloaded or handled as in previous scripts)\n",
    "PUNKT_RESOURCE_ID = 'tokenizers/punkt'\n",
    "try:\n",
    "    nltk.data.find(PUNKT_RESOURCE_ID)\n",
    "except LookupError: # Catches both DownloadError and general LookupError\n",
    "    print(f\"NLTK 'punkt' tokenizer not found ('{PUNKT_RESOURCE_ID}'). Attempting to download...\")\n",
    "    try:\n",
    "        nltk.download('punkt', quiet=True)\n",
    "        nltk.data.find(PUNKT_RESOURCE_ID) # Verify after download\n",
    "        print(\"'punkt' downloaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download or verify 'punkt': {e}\")\n",
    "        print(\"Please ensure an internet connection and NLTK can write to its data directory.\")\n",
    "        print(\"You might need to run Python with administrator privileges or manually download 'punkt'.\")\n",
    "        exit()\n",
    "# --- End NLTK Resource Download Check ---\n",
    "\n",
    "# Initialize Pyphen for English\n",
    "try:\n",
    "    dic = pyphen.Pyphen(lang='en_US')\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing pyphen for 'en_US': {e}\")\n",
    "    print(\"Please ensure pyphen dictionaries are correctly installed.\")\n",
    "    exit()\n",
    "\n",
    "def count_syllables(word):\n",
    "    \"\"\"Counts syllables in a word using pyphen.\"\"\"\n",
    "    word = word.lower().strip(string.punctuation)\n",
    "    if not word:\n",
    "        return 0\n",
    "    \n",
    "    hyphenated_word = dic.inserted(word)\n",
    "    if not hyphenated_word: \n",
    "        return 1 if len(word) > 0 else 0\n",
    "        \n",
    "    num_syllables = len(hyphenated_word.split('-'))\n",
    "    \n",
    "    if len(word) <= 3 and num_syllables == 1:\n",
    "        return 1\n",
    "    return num_syllables\n",
    "\n",
    "\n",
    "def get_gunning_fog_index(text):\n",
    "    \"\"\"Calculates the Gunning Fog Index for a given text.\"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return np.nan # Use np.nan for missing/invalid inputs\n",
    "\n",
    "    # 1. Sentence Segmentation\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    num_sentences = len(sentences)\n",
    "    if num_sentences == 0:\n",
    "        return np.nan\n",
    "\n",
    "    # 2. Word Tokenization and Counting\n",
    "    words = []\n",
    "    for sentence in sentences:\n",
    "        sentence_words = nltk.word_tokenize(sentence)\n",
    "        words.extend([word for word in sentence_words if word.isalnum()]) \n",
    "\n",
    "    num_words = len(words)\n",
    "    if num_words == 0:\n",
    "        return np.nan\n",
    "\n",
    "    # 3. Average Words Per Sentence (AWSL)\n",
    "    avg_words_per_sentence = num_words / num_sentences\n",
    "\n",
    "    # 4. Complex Words (3+ syllables)\n",
    "    complex_word_count = 0\n",
    "    for word in words:\n",
    "        if len(word) <= 2: \n",
    "            continue\n",
    "        syllables = count_syllables(word)\n",
    "        if syllables >= 3:\n",
    "            complex_word_count += 1\n",
    "    \n",
    "    # 5. Percentage of Complex Words (PCW)\n",
    "    percentage_complex_words = (complex_word_count / num_words) * 100 if num_words > 0 else 0\n",
    "\n",
    "    # 6. Gunning Fog Index\n",
    "    gunning_fog = 0.4 * (avg_words_per_sentence + percentage_complex_words)\n",
    "    \n",
    "    return gunning_fog\n",
    "\n",
    "# --- Main script execution (Modified for Combined Text per Doc_id) ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the CSV file\n",
    "    try:\n",
    "        df = pd.read_csv('llm_data3.csv')\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: 'llm_data3.csv' not found. Make sure the file is in the same directory.\")\n",
    "        exit()\n",
    "\n",
    "    doc_level_gfi_results = []\n",
    "\n",
    "    print(\"Processing documents to calculate Gunning Fog Index on combined company speech...\")\n",
    "\n",
    "    # Group by Doc_id to process each earnings call\n",
    "    for doc_id, group in df.groupby('Doc_id'):\n",
    "        # Filter for company representative speech (Speaker_Type == 1)\n",
    "        company_speech_segments = group[group['Speaker_Type'] == 1]['Speech'].astype(str).tolist()\n",
    "        \n",
    "        if not company_speech_segments:\n",
    "            # No company speech in this Doc_id, append NaN or skip\n",
    "            doc_level_gfi_results.append({'Doc_id': doc_id, 'Gunning_Fog_Combined': np.nan})\n",
    "            # print(f\"No company speech found for Doc_id: {doc_id}\")\n",
    "            continue\n",
    "            \n",
    "        # Concatenate all company speech segments into a single string\n",
    "        combined_company_text = \" \".join(company_speech_segments)\n",
    "        \n",
    "        # Calculate Gunning Fog Index on the combined text\n",
    "        # The get_gunning_fog_index function handles empty or very short combined_company_text\n",
    "        gfi_score_for_doc = get_gunning_fog_index(combined_company_text)\n",
    "        \n",
    "        doc_level_gfi_results.append({'Doc_id': doc_id, 'Gunning_Fog_Combined': gfi_score_for_doc})\n",
    "\n",
    "    print(\"Gunning Fog Index calculation complete for all documents.\")\n",
    "\n",
    "    if not doc_level_gfi_results:\n",
    "        print(\"No documents processed or no company speech found in any document. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "    # Convert list of results to a DataFrame\n",
    "    final_gfi_df = pd.DataFrame(doc_level_gfi_results)\n",
    "    \n",
    "    # Rename the column for clarity (optional, but good practice)\n",
    "    final_gfi_df.rename(columns={'Gunning_Fog_Combined': 'Gunning_Fog'}, inplace=True)\n",
    "\n",
    "    # Handle potential NaN values if a Doc_id had no company speech or all speeches resulted in None\n",
    "    final_gfi_df.dropna(subset=['unning_Fog'], inplace=True) # Or your chosen column name\n",
    "\n",
    "    # Save the results to a new CSV file\n",
    "    output_filename = 'doc_gunning_fog_scores_combined_fix.csv'\n",
    "    final_gfi_df.to_csv(output_filename, index=False)\n",
    "\n",
    "    print(f\"\\nGunning Fog Index scores (calculated on combined text per Doc_id) saved to '{output_filename}'\")\n",
    "    print(\"\\nFirst few rows of the output:\")\n",
    "    print(final_gfi_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c88fdb7",
   "metadata": {},
   "source": [
    "VADER Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12a873de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing documents to calculate VADER sentiment on combined company speech...\n",
      "VADER sentiment calculation complete for all documents.\n",
      "\n",
      "VADER sentiment scores (calculated on combined text per Doc_id) saved to 'doc_vader_sentiment_scores_fix.csv'\n",
      "\n",
      "First few rows of the output:\n",
      "    Doc_id  VADER_Negative_Combined  VADER_Neutral_Combined  \\\n",
      "0  1943275                    0.031                   0.817   \n",
      "1  2038813                    0.026                   0.845   \n",
      "2  2053228                    0.025                   0.802   \n",
      "3  2053230                    0.017                   0.820   \n",
      "4  2056621                    0.013                   0.817   \n",
      "\n",
      "   VADER_Positive_Combined  VADER_Compound_Combined  \n",
      "0                    0.152                   1.0000  \n",
      "1                    0.129                   0.9998  \n",
      "2                    0.173                   1.0000  \n",
      "3                    0.164                   0.9999  \n",
      "4                    0.170                   1.0000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import numpy as np # For np.nan\n",
    "\n",
    "# Initialize VADER so you don't have to do it for each row\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_vader_sentiment_scores(text):\n",
    "    \"\"\"\n",
    "    Calculates VADER sentiment scores for a given text.\n",
    "    Returns a dictionary with neg, neu, pos, and compound scores.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        # Return a dictionary with NaNs for empty/invalid input\n",
    "        return {'neg': np.nan, 'neu': np.nan, 'pos': np.nan, 'compound': np.nan}\n",
    "    \n",
    "    vs = analyzer.polarity_scores(text)\n",
    "    return vs\n",
    "\n",
    "# --- Main script execution (Modified for Combined Text per Doc_id) ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the CSV file\n",
    "    try:\n",
    "        df = pd.read_csv('llm_data3.csv')\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: 'llm_data3.csv' not found. Make sure the file is in the same directory.\")\n",
    "        exit()\n",
    "\n",
    "    doc_level_vader_results = []\n",
    "\n",
    "    print(\"Processing documents to calculate VADER sentiment on combined company speech...\")\n",
    "\n",
    "    # Group by Doc_id to process each earnings call\n",
    "    for doc_id, group in df.groupby('Doc_id'):\n",
    "        # Filter for company representative speech (Speaker_Type == 1)\n",
    "        company_speech_segments = group[group['Speaker_Type'] == 1]['Speech'].astype(str).tolist()\n",
    "        \n",
    "        if not company_speech_segments:\n",
    "            # No company speech in this Doc_id, append a record with NaNs\n",
    "            vader_scores_for_doc = {'neg': np.nan, 'neu': np.nan, 'pos': np.nan, 'compound': np.nan}\n",
    "            # print(f\"No company speech found for Doc_id: {doc_id}\")\n",
    "        else:\n",
    "            # Concatenate all company speech segments into a single string\n",
    "            combined_company_text = \" \".join(company_speech_segments)\n",
    "            \n",
    "            # Calculate VADER sentiment on the combined text\n",
    "            # The get_vader_sentiment_scores function handles empty or very short combined_company_text\n",
    "            vader_scores_for_doc = get_vader_sentiment_scores(combined_company_text)\n",
    "        \n",
    "        # Add Doc_id to the scores dictionary and append to results list\n",
    "        result_record = {'Doc_id': doc_id, **vader_scores_for_doc}\n",
    "        doc_level_vader_results.append(result_record)\n",
    "\n",
    "    print(\"VADER sentiment calculation complete for all documents.\")\n",
    "\n",
    "    if not doc_level_vader_results:\n",
    "        print(\"No documents processed or no company speech found in any document. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "    # Convert list of results to a DataFrame\n",
    "    final_vader_df = pd.DataFrame(doc_level_vader_results)\n",
    "    \n",
    "    # Rename columns for clarity in the final output\n",
    "    final_vader_df.rename(columns={\n",
    "        'neg': 'VADER_Negative_Combined',\n",
    "        'neu': 'VADER_Neutral_Combined',\n",
    "        'pos': 'VADER_Positive_Combined',\n",
    "        'compound': 'VADER_Compound_Combined'\n",
    "        # Or use 'Average_VADER_...' if you want to maintain consistency with previous naming conventions\n",
    "        # e.g., 'Average_VADER_Negative' if that's what your regression scripts expect.\n",
    "        # For now, '_Combined' clearly indicates the new method.\n",
    "    }, inplace=True)\n",
    "\n",
    "    # Handle potential NaN values if a Doc_id had no company speech or all speeches resulted in NaN scores\n",
    "    # Dropping rows where the compound score is NaN is a common approach.\n",
    "    final_vader_df.dropna(subset=['VADER_Compound_Combined'], inplace=True)\n",
    "\n",
    "    # Save the results to a new CSV file\n",
    "    output_filename = 'doc_vader_sentiment_scores_fix.csv'\n",
    "    final_vader_df.to_csv(output_filename, index=False)\n",
    "\n",
    "    print(f\"\\nVADER sentiment scores (calculated on combined text per Doc_id) saved to '{output_filename}'\")\n",
    "    print(\"\\nFirst few rows of the output:\")\n",
    "    print(final_vader_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f014f3e",
   "metadata": {},
   "source": [
    "Lexicon Based Analysis, LM Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "409ff4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 2345 LM 'Negative' words.\n",
      "Successfully loaded 347 LM 'Positive' words.\n",
      "Successfully loaded 297 LM 'Uncertainty' words.\n",
      "Successfully loaded 903 LM 'Litigious' words.\n",
      "Successfully loaded 19 LM 'Strong_Modal' words.\n",
      "Successfully loaded 27 LM 'Weak_Modal' words.\n",
      "Successfully loaded 184 LM 'Constraining' words.\n",
      "Processing documents to calculate lexicon scores on combined company speech...\n",
      "Lexicon score calculation complete for all documents.\n",
      "\n",
      "Lexicon-based scores (calculated on combined text per Doc_id) saved to 'doc_lexicon_based_scores_fix.csv'\n",
      "\n",
      "First few rows of the output:\n",
      "    Doc_id  lm_negative_ratio  lm_positive_ratio  lm_uncertainty_ratio  \\\n",
      "0  1943275           0.006918           0.013836              0.007588   \n",
      "1  2038813           0.006372           0.008850              0.014159   \n",
      "2  2053228           0.009234           0.021622              0.009685   \n",
      "3  2053230           0.007192           0.019521              0.008562   \n",
      "4  2056621           0.004623           0.020704              0.008643   \n",
      "\n",
      "   lm_litigious_ratio  lm_strong_modal_ratio  lm_weak_modal_ratio  \\\n",
      "0            0.000000               0.007811             0.002901   \n",
      "1            0.004248               0.009912             0.006018   \n",
      "2            0.000000               0.003829             0.001802   \n",
      "3            0.001027               0.007534             0.002740   \n",
      "4            0.000402               0.011457             0.002211   \n",
      "\n",
      "   lm_constraining_ratio  custom_weak_modal_ratio  custom_hedging_word_ratio  \\\n",
      "0               0.000000                 0.009150                   0.019192   \n",
      "1               0.001770                 0.007434                   0.020885   \n",
      "2               0.000676                 0.005180                   0.016441   \n",
      "3               0.001712                 0.002740                   0.015411   \n",
      "4               0.001608                 0.002814                   0.015075   \n",
      "\n",
      "   custom_hedging_phrase_ratio  total_words_for_lex_analysis  \n",
      "0                     0.009373                          4481  \n",
      "1                     0.012743                          2825  \n",
      "2                     0.006081                          4440  \n",
      "3                     0.007192                          2920  \n",
      "4                     0.010452                          4975  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# --- NLTK Resource Download Check (ensure 'punkt' is available) ---\n",
    "# (Assuming 'punkt' is already downloaded or handled as in previous scripts)\n",
    "PUNKT_RESOURCE_ID = 'tokenizers/punkt'\n",
    "try:\n",
    "    nltk.data.find(PUNKT_RESOURCE_ID)\n",
    "except LookupError:\n",
    "    print(f\"NLTK 'punkt' tokenizer not found ('{PUNKT_RESOURCE_ID}'). Attempting to download...\")\n",
    "    try:\n",
    "        nltk.download('punkt', quiet=True)\n",
    "        nltk.data.find(PUNKT_RESOURCE_ID)\n",
    "        print(\"'punkt' downloaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download or verify 'punkt': {e}\")\n",
    "        exit()\n",
    "# --- End NLTK Resource Download Check ---\n",
    "\n",
    "# --- Define Lexicons ---\n",
    "LM_DICTIONARY_FILE = \"Loughran-McDonald_MasterDictionary_1993-2024.csv\"\n",
    "LM_CATEGORIES_TO_LOAD = {\n",
    "    \"Negative\": \"lm_negative\",\n",
    "    \"Positive\": \"lm_positive\",\n",
    "    \"Uncertainty\": \"lm_uncertainty\",\n",
    "    \"Litigious\": \"lm_litigious\",\n",
    "    \"Strong_Modal\": \"lm_strong_modal\",\n",
    "    \"Weak_Modal\": \"lm_weak_modal\",\n",
    "    \"Constraining\": \"lm_constraining\"\n",
    "}\n",
    "LM_WORD_LISTS = {key: set() for key in LM_CATEGORIES_TO_LOAD.values()}\n",
    "\n",
    "def load_lm_category_words(lm_df, lm_category_column_name):\n",
    "    words_set = set()\n",
    "    if lm_category_column_name in lm_df.columns:\n",
    "        word_series = None\n",
    "        if lm_df[lm_category_column_name].dtype == 'bool':\n",
    "            word_series = lm_df[lm_df[lm_category_column_name]]['Word']\n",
    "        elif pd.api.types.is_numeric_dtype(lm_df[lm_category_column_name]):\n",
    "            word_series = lm_df[lm_df[lm_category_column_name] > 0]['Word']\n",
    "        else:\n",
    "            try:\n",
    "                word_series = lm_df[lm_df[lm_category_column_name].fillna(False).astype(bool)]['Word']\n",
    "            except ValueError:\n",
    "                 print(f\"Warning: LM Column '{lm_category_column_name}' type {lm_df[lm_category_column_name].dtype} failed bool conversion.\")\n",
    "        \n",
    "        if word_series is not None and not word_series.empty:\n",
    "            if 'Word' in lm_df.columns:\n",
    "                words_set = set(word_series.astype(str).str.lower().tolist()) # Ensure words are strings and lowercased\n",
    "                print(f\"Successfully loaded {len(words_set)} LM '{lm_category_column_name}' words.\")\n",
    "            else:\n",
    "                print(f\"Warning: 'Word' column missing in LM dict for '{lm_category_column_name}'.\")\n",
    "        elif word_series is None:\n",
    "             pass\n",
    "        else:\n",
    "            print(f\"Warning: No words found for LM category '{lm_category_column_name}'.\")\n",
    "    else:\n",
    "        print(f\"Warning: LM Category column '{lm_category_column_name}' not found.\")\n",
    "    return words_set\n",
    "\n",
    "try:\n",
    "    lm_df_full = pd.read_csv(LM_DICTIONARY_FILE)\n",
    "    if 'Word' in lm_df_full.columns:\n",
    "        # It's better to lowercase the words from the dictionary once, rather than in load_lm_category_words\n",
    "        # lm_df_full['Word'] = lm_df_full['Word'].astype(str).str.lower() \n",
    "        # Actually, let's do it inside load_lm_category_words to ensure it's applied per series\n",
    "        pass\n",
    "    else:\n",
    "        print(f\"CRITICAL WARNING: 'Word' column not found in {LM_DICTIONARY_FILE}.\")\n",
    "\n",
    "    for lm_col_name, score_prefix in LM_CATEGORIES_TO_LOAD.items():\n",
    "        LM_WORD_LISTS[score_prefix] = load_lm_category_words(lm_df_full, lm_col_name)\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: LM dictionary '{LM_DICTIONARY_FILE}' not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Could not parse LM dictionary '{LM_DICTIONARY_FILE}': {e}\")\n",
    "\n",
    "WEAK_MODAL_VERBS_CUSTOM = {\"may\", \"might\", \"could\", \"would\", \"should\"}\n",
    "HEDGING_LEXICON_CUSTOM = {\n",
    "    \"about\", \"almost\", \"apparently\", \"approximately\", \"around\", \"assume\", \"assumed\", \"assumes\",\n",
    "    \"assumption\", \"believe\", \"believed\", \"believes\", \"broadly\", \"cautiously\", \"conceivably\",\n",
    "    \"could\", \"estimate\", \"estimated\", \"estimates\", \"fairly\", \"feel\", \"felt\", \"frequently\",\n",
    "    \"generally\", \"guess\", \"guessed\", \"guesses\", \"hopefully\", \"indicate\", \"indicated\", \"indicates\",\n",
    "    \"largely\", \"likely\", \"mainly\", \"may\", \"maybe\", \"might\", \"mostly\", \"often\", \"overall\",\n",
    "    \"partially\", \"perhaps\", \"plausibly\", \"possibly\", \"potential\", \"potentially\", \"presumably\",\n",
    "    \"presume\", \"probable\", \"probably\", \"quite\", \"rather\", \"relatively\", \"roughly\", \"seems\", \"seemed\",\n",
    "    \"should\", \"sometimes\", \"somewhat\", \"suggest\", \"suggested\", \"suggests\", \"suppose\", \"supposed\",\n",
    "    \"tend\", \"tended\", \"tends\", \"typically\", \"uncertain\", \"unclear\", \"unlikely\", \"usually\", \"virtually\",\n",
    "    \"would\", \"according to\", \"appear to be\", \"appears to be\", \"as far as i can tell\", \"as far as we know\",\n",
    "    \"based on\", \"can be seen as\", \"could be\", \"doubtful that\", \"effective as\", \"expected to\",\n",
    "    \"expected to be\", \"feels like\", \"from our perspective\", \"highly likely\", \"i believe\", \"i feel\",\n",
    "    \"i think\", \"in general\", \"in most cases\", \"in my opinion\", \"in our opinion\", \"in our view\",\n",
    "    \"it appears\", \"it could be that\", \"it is conceivable\", \"it is likely\", \"it is possible\",\n",
    "    \"it is probable\", \"it may be\", \"it might be\", \"it seems\", \"it seems that\", \"it would appear\",\n",
    "    \"it would seem\", \"looks like\", \"may be\", \"might be\", \"more or less\", \"my impression is\",\n",
    "    \"not necessarily\", \"on balance\", \"our understanding is\", \"point of view\", \"points to\",\n",
    "    \"possible that\", \"presumed to be\", \"seems to\", \"so to speak\", \"suggests that\", \"tend to\",\n",
    "    \"tends to\", \"there is a chance\", \"there is a possibility\", \"to some extent\", \"we assume\",\n",
    "    \"we believe\", \"we estimate\", \"we feel\", \"we guess\", \"we suggest\", \"we think\", \"will likely\",\n",
    "    \"would argue\", \"would assume\", \"would guess\", \"would suggest\"\n",
    "}\n",
    "HEDGING_WORDS_CUSTOM = {word for word in HEDGING_LEXICON_CUSTOM if ' ' not in word}\n",
    "HEDGING_PHRASES_CUSTOM = {phrase for phrase in HEDGING_LEXICON_CUSTOM if ' ' in phrase}\n",
    "# --- End Define Lexicons ---\n",
    "\n",
    "def preprocess_text_for_lexicon(text):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return [], \"\" \n",
    "    lower_text = text.lower()\n",
    "    tokens = re.findall(r'\\b\\w+\\b', lower_text)\n",
    "    return tokens, lower_text\n",
    "\n",
    "def calculate_lexicon_scores(speech_text):\n",
    "    scores = {f\"{prefix}_ratio\": np.nan for prefix in LM_WORD_LISTS.keys()}\n",
    "    scores.update({\n",
    "        'custom_weak_modal_ratio': np.nan,\n",
    "        'custom_hedging_word_ratio': np.nan,\n",
    "        'custom_hedging_phrase_ratio': np.nan,\n",
    "        'total_words_for_lex_analysis': 0\n",
    "    })\n",
    "\n",
    "    if not isinstance(speech_text, str) or not speech_text.strip():\n",
    "        return scores\n",
    "\n",
    "    tokens, lower_speech_text = preprocess_text_for_lexicon(speech_text)\n",
    "    num_total_words = len(tokens)\n",
    "    scores['total_words_for_lex_analysis'] = num_total_words\n",
    "\n",
    "    if num_total_words == 0:\n",
    "        for key in scores:\n",
    "            if key.endswith(\"_ratio\"): scores[key] = 0.0\n",
    "        return scores\n",
    "\n",
    "    for score_prefix, word_list in LM_WORD_LISTS.items():\n",
    "        if word_list: \n",
    "            count = sum(1 for token in tokens if token in word_list)\n",
    "            scores[f\"{score_prefix}_ratio\"] = count / num_total_words\n",
    "        else:\n",
    "            scores[f\"{score_prefix}_ratio\"] = 0.0 \n",
    "\n",
    "    weak_modal_custom_count = sum(1 for token in tokens if token in WEAK_MODAL_VERBS_CUSTOM)\n",
    "    scores['custom_weak_modal_ratio'] = weak_modal_custom_count / num_total_words\n",
    "\n",
    "    hedging_word_custom_count = sum(1 for token in tokens if token in HEDGING_WORDS_CUSTOM)\n",
    "    scores['custom_hedging_word_ratio'] = hedging_word_custom_count / num_total_words\n",
    "\n",
    "    hedging_phrase_custom_count = 0\n",
    "    for phrase in HEDGING_PHRASES_CUSTOM:\n",
    "        hedging_phrase_custom_count += lower_speech_text.count(phrase)\n",
    "    scores['custom_hedging_phrase_ratio'] = hedging_phrase_custom_count / num_total_words\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# --- Main script execution (Modified for Combined Text per Doc_id) ---\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        df = pd.read_csv('llm_data3.csv') \n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: 'llm_data3.csv' not found. Make sure the earnings call data file is in the same directory.\")\n",
    "        exit()\n",
    "\n",
    "    doc_level_lexicon_results = []\n",
    "\n",
    "    print(\"Processing documents to calculate lexicon scores on combined company speech...\")\n",
    "\n",
    "    for doc_id, group in df.groupby('Doc_id'):\n",
    "        company_speech_segments = group[group['Speaker_Type'] == 1]['Speech'].astype(str).tolist()\n",
    "        \n",
    "        current_doc_scores = {} # To hold scores for the current document\n",
    "        if not company_speech_segments:\n",
    "            # Initialize all score keys with NaN if no company speech\n",
    "            # This uses the keys from the `calculate_lexicon_scores` default return structure\n",
    "            temp_scores = calculate_lexicon_scores(\"\") # Get default structure\n",
    "            for key in temp_scores:\n",
    "                if key.endswith(\"_ratio\"):\n",
    "                    current_doc_scores[key] = np.nan\n",
    "                elif key == 'total_words_for_lex_analysis':\n",
    "                    current_doc_scores[key] = 0\n",
    "            # print(f\"No company speech found for Doc_id: {doc_id}\")\n",
    "        else:\n",
    "            combined_company_text = \" \".join(company_speech_segments)\n",
    "            current_doc_scores = calculate_lexicon_scores(combined_company_text)\n",
    "        \n",
    "        result_record = {'Doc_id': doc_id, **current_doc_scores}\n",
    "        doc_level_lexicon_results.append(result_record)\n",
    "\n",
    "    print(\"Lexicon score calculation complete for all documents.\")\n",
    "\n",
    "    if not doc_level_lexicon_results:\n",
    "        print(\"No documents processed or no company speech found. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "    final_lexicon_df = pd.DataFrame(doc_level_lexicon_results)\n",
    "    \n",
    "    # Rename columns if needed, e.g., append '_Combined' or adjust to match regression script expectations\n",
    "    # For simplicity, current names like 'lm_negative_ratio' already imply they are for the document\n",
    "    # if this script's output is the only source.\n",
    "    # Example renaming:\n",
    "    # new_column_names = {'lm_negative_ratio': 'lm_negative_ratio_combined', ...}\n",
    "    # final_lexicon_df.rename(columns=new_column_names, inplace=True)\n",
    "    \n",
    "    # Define which columns are the actual ratio scores for checking NaNs\n",
    "    # This can be derived from the keys in the 'scores' dict in calculate_lexicon_scores\n",
    "    score_ratio_columns = [key for key in calculate_lexicon_scores(\"\").keys() if key.endswith(\"_ratio\")]\n",
    "    final_lexicon_df.dropna(subset=score_ratio_columns, how='all', inplace=True)\n",
    "\n",
    "\n",
    "    output_filename = 'doc_lexicon_based_scores_fix.csv'\n",
    "    final_lexicon_df.to_csv(output_filename, index=False)\n",
    "\n",
    "    print(f\"\\nLexicon-based scores (calculated on combined text per Doc_id) saved to '{output_filename}'\")\n",
    "    print(\"\\nFirst few rows of the output:\")\n",
    "    print(final_lexicon_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
